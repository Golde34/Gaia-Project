{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## -----------------------------\n",
    "## 1. MODEL\n",
    "## -----------------------------\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None           # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True    # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "## -----------------------------\n",
    "## 2. DATASET\n",
    "## -----------------------------\n",
    "from datasets import load_dataset\n",
    "document = load_dataset(\"xDAN-datasets/medical_meadow_wikidoc_patient_information_6k\", split=\"train\")\n",
    "document.to_csv(\"./dataset/rag_data.csv\")\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./dataset\").load_data()\n",
    "\n",
    "## -----------------------------\n",
    "## 3. EMBEDDINGS & SETTINGS\n",
    "## -----------------------------\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]\n",
    "\n",
    "## -----------------------------\n",
    "## 4. TẠO INDEX VỚI CHROMADB\n",
    "## -----------------------------\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "# Tạo ChromaVectorStore, chỉ rõ nơi lưu (persist_directory) và tên collection\n",
    "chroma_store = ChromaVectorStore(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"medical_collection\"\n",
    ")\n",
    "\n",
    "# Tạo Index, sử dụng ChromaVectorStore\n",
    "# (Ví dụ này, tất cả documents đều được index chung)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=Settings.transformations,\n",
    "    vector_store=chroma_store\n",
    ")\n",
    "\n",
    "## -----------------------------\n",
    "## 5. TẠO QUERY ENGINE CƠ BẢN\n",
    "## -----------------------------\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "query_str = \"What causes Alstrom syndrome?\"\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "## -----------------------------\n",
    "## 6. RAG (Retriever + LLM)\n",
    "## -----------------------------\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=2)\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "query_str = \"What causes Alstrom syndrome?\"\n",
    "hyde_query_engine = TransformQueryEngine(vector_query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "## -----------------------------\n",
    "## 7. THÊM SEMANTIC ROUTER\n",
    "## -----------------------------\n",
    "\"\"\"\n",
    "  Mục đích: Tạo một router “semantic” để phân loại câu hỏi vào 2 route:\n",
    "    1) \"product\" (câu hỏi về sản phẩm hay domain cụ thể)\n",
    "    2) \"chitchat\" (trò chuyện thường)\n",
    "  \n",
    "  Thông thường, ta sẽ có 2 index khác nhau (product_index và chitchat_index).\n",
    "  Trong code demo này, mình dùng chung 'index' để minh họa. \n",
    "  Thực tế bạn nên tách tài liệu nào liên quan “product” sang 1 index, \n",
    "  còn tài liệu “chitchat” (hay general) vào 1 index khác, rồi router quyết định route.\n",
    "\"\"\"\n",
    "# Giả sử ta có \"product_index\" & \"chitchat_index\". \n",
    "# Ở đây demo cùng index => \"product_index = index\", \"chitchat_index = index\"\n",
    "\n",
    "product_index = index  # thay bằng index chứa tài liệu product\n",
    "chitchat_index = index # thay bằng index chứa tài liệu chitchat\n",
    "\n",
    "product_query_engine = product_index.as_query_engine()\n",
    "chitchat_query_engine = chitchat_index.as_query_engine()\n",
    "\n",
    "##\n",
    "## 7.1 Tạo \"tools\" cho 2 route: product_tool, chitchat_tool\n",
    "##\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "product_tool = QueryEngineTool(\n",
    "    query_engine=product_query_engine,\n",
    "    name=\"product_tool\",\n",
    "    description=\"Use this tool for answering questions about PRODUCT domain.\"\n",
    ")\n",
    "chitchat_tool = QueryEngineTool(\n",
    "    query_engine=chitchat_query_engine,\n",
    "    name=\"chitchat_tool\",\n",
    "    description=\"Use this tool for normal chitchat or everyday topics.\"\n",
    ")\n",
    "\n",
    "tools = [product_tool, chitchat_tool]\n",
    "\n",
    "##\n",
    "## 7.2 Tạo RouterQueryEngine\n",
    "##\n",
    "# RouterQueryEngine sẽ dùng LLM để “route” câu hỏi đến tool thích hợp.\n",
    "# Nó tự sinh prompt: “Which tool nên được dùng cho query này?” \n",
    "#\n",
    "# Tùy phiên bản, import có thể thay đổi:\n",
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "\n",
    "router_query_engine = RouterQueryEngine.from_tools(\n",
    "    tools=tools,\n",
    "    llm=llm,  # LLM sử dụng để phân tích và route\n",
    "    default_tool=chitchat_tool  # route mặc định nếu model không chắc\n",
    ")\n",
    "\n",
    "##\n",
    "## 7.3 Query qua router\n",
    "##\n",
    "print(\"=== Demo: Query về product ===\")\n",
    "product_question = \"Give me specifications of the new medical device\"\n",
    "router_response_1 = router_query_engine.query(product_question)\n",
    "display(Markdown(f\"<b>Product Route Q:</b> {product_question}\"))\n",
    "display(Markdown(f\"<b>Answer:</b> {router_response_1}\"))\n",
    "\n",
    "print(\"\\n=== Demo: Query chitchat ===\")\n",
    "chitchat_question = \"How is the weather today?\"\n",
    "router_response_2 = router_query_engine.query(chitchat_question)\n",
    "display(Markdown(f\"<b>Chitchat Route Q:</b> {chitchat_question}\"))\n",
    "display(Markdown(f\"<b>Answer:</b> {router_response_2}\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
