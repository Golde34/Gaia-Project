{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name__ = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "model__ = SentenceTransformer(embedding_model_name__)\n",
    "\n",
    "def embed(texts, device = 'cpu'):\n",
    "    embeddings = model__.encode(sentences=texts, device=device, normalize_embeddings=True)\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import copy\n",
    "\n",
    "CHROMA_CLIENT = chromadb.PersistentClient(path='vectordb')\n",
    "COLLECTION = CHROMA_CLIENT.get_or_create_collection(name = 'gaia', \n",
    "                                                    metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "RERANKER = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"jinaai/jina-reranker-v2-base-multilingual\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "TEXT_SPLITTER = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1014,\n",
    "    chunk_overlap=256,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Add a list of texts and their associated metadata to a vector database.\n",
    "#   Parameters:\n",
    "#     texts (list of str): A list of texts to be added to the vector database.\n",
    "#     metadatas (list of dict): A list of metadata dictionaries corresponding to each text. Each dictionary should \n",
    "#                               include at least a 'file_id' key.\n",
    "#     device (str): The device used for embedding computation (e.g., \"cpu\" or \"gpu\").\n",
    "def add_texts_to_vectordb(texts, metadatas, device):\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        \n",
    "        # Split text into chunks\n",
    "        splited_texts = TEXT_SPLITTER.split_text(texts[i])\n",
    "        file_id = metadatas[i]['file_id']\n",
    "        ids = [file_id + \"_\" + str(j) for j in range(len(splited_texts))]\n",
    "        \n",
    "        # Get embeddings from chunks\n",
    "        embeddings = embed(texts=splited_texts, device=device)\n",
    "        file_metadatas = [copy.deepcopy(metadatas[i]) for _ in range(len(splited_texts))]\n",
    "        \n",
    "        # Apply sentence window to save bigger context in a chunk\n",
    "        for idx in range(len((file_metadatas))):\n",
    "            if idx == 0:\n",
    "                sentence_window_doc = splited_texts[0] + '\\n' + splited_texts[1]\n",
    "            elif idx == len(splited_texts) - 1:\n",
    "                sentence_window_doc = splited_texts[-2] + '\\n' + splited_texts[-1]\n",
    "            else:\n",
    "                sentence_window_doc = splited_texts[idx - 1] + '\\n' + splited_texts[idx] + '\\n' + splited_texts[idx + 1]\n",
    "                \n",
    "            file_metadatas[idx]['sentence_window_document'] = sentence_window_doc\n",
    "        \n",
    "        # Add data to vector DB\n",
    "        COLLECTION.add(\n",
    "            ids = ids,\n",
    "            documents = splited_texts,\n",
    "            embeddings = embeddings,\n",
    "            metadatas = file_metadatas\n",
    "        )\n",
    "    \n",
    "        print(f\"\\n===== ADD {len(splited_texts)} DOCUMENTS TO VECTORDB ===============\")\n",
    "        print(\"Length original text:\", len(texts[i]))\n",
    "        print(\"Metadatas:\", file_metadatas[0])\n",
    "        \n",
    "\n",
    "# Query a vector database to retrieve and rank documents based on similarity to the given query.\n",
    "#   Parameters:\n",
    "#     query (str): The text query to search for in the vector database.\n",
    "#     device (str): The device on which the embedding and querying should be performed.\n",
    "#     n_vectordb (int, optional): The number of top similar documents to retrieve from the vector database (default is 10).\n",
    "#     n_rerank (int, optional): The number of top-ranked documents to return after reranking (default is 3).\n",
    "#   Returns:\n",
    "#     dict: A dictionary containing:\n",
    "#         - 'documents' (list of str): The top-ranked documents after reranking.\n",
    "#         - 'scores' (list of float): The similarity scores of the top-ranked documents.\n",
    "#         - 'file_names' (list of str): The file names corresponding to the top-ranked documents.\n",
    "def query_vectordb(query, device, n_vectordb = 10, n_rerank = 3):\n",
    "    \n",
    "    # Get embedding from user's query\n",
    "    queryy: list[str] = [query]\n",
    "    query_embedding = embed(texts=queryy, device=device)\n",
    "    \n",
    "    # Query in vector DB\n",
    "    vectordb_results = COLLECTION.query(\n",
    "        query_embeddings = query_embedding,\n",
    "        n_results = n_vectordb\n",
    "    )\n",
    "    \n",
    "    sentence_window_documents = [t['sentence_window_document'] for t in vectordb_results['metadatas'][0]]\n",
    "    file_names = [t['file_name'] for t in vectordb_results['metadatas'][0]]\n",
    "    \n",
    "    sim_sentences = vectordb_results['documents'][0]\n",
    "    sim_scores = vectordb_results['distances'][0]\n",
    "    \n",
    "    print(\"\\n===== VECTORDB QUERY RESULTS ==============\")\n",
    "    for sentence, score in zip(sim_sentences, sim_scores):\n",
    "        print(f\"\\n=== Documents ({score}):\", sentence)\n",
    "    \n",
    "    query_doc_pairs = [[query, s] for s in sim_sentences]\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True)\n",
    "    RERANKER = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'jinaai/jina-reranker-v2-base-multilingual',\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    RERANKER.to(device)\n",
    "    RERANKER.eval()\n",
    "\n",
    "    # Tokenize the input query-document pairs\n",
    "    tokenized_pairs = [tokenizer(query, doc, return_tensors=\"pt\", padding=True, truncation=True) for query, doc in query_doc_pairs]\n",
    "\n",
    "    # Compute rerank scores\n",
    "    rerank_scores = []\n",
    "    for tokenized_input in tokenized_pairs:\n",
    "        with torch.no_grad():\n",
    "            outputs = RERANKER(**tokenized_input.to(device))\n",
    "            rerank_scores.append(outputs.logits.squeeze().item())\n",
    "\n",
    "    # Sort by rerank scores\n",
    "    sorted_indices = sorted(range(len(rerank_scores)), key=lambda i: rerank_scores[i], reverse=True)\n",
    "\n",
    "    final_results = {\n",
    "        'documents': [sentence_window_documents[i] for idx, i in enumerate(sorted_indices) if idx < n_rerank],\n",
    "        'scores': [rerank_scores[i] for idx, i in enumerate(sorted_indices) if idx < n_rerank],\n",
    "        \"file_names\": [file_names[i] for idx, i in enumerate(sorted_indices) if idx < n_rerank]\n",
    "    }\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def get_all_from_vectordb():\n",
    "    all_data = COLLECTION.get(\n",
    "        include=['documents', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def delete_from_vectordb(delete_id):\n",
    "    ids_to_delete = []\n",
    "    \n",
    "    all_data = COLLECTION.get()\n",
    "    \n",
    "    # Get all data where \"file_id\" in metadatas equals delete_id\n",
    "    for idx in range(len(all_data['ids'])):\n",
    "        id = all_data['ids'][idx]\n",
    "        metadata = all_data['metadatas'][idx]\n",
    "        \n",
    "        if metadata['file_id'] == delete_id:\n",
    "            ids_to_delete.append(id)\n",
    "            \n",
    "    COLLECTION.delete(ids = ids_to_delete)\n",
    "        \n",
    "    \n",
    "    print(\"\\n===== DELETE FROM VECTORDB:\")\n",
    "    print(\"Number of deleted docs:\", len(ids_to_delete))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from werkzeug.datastructures import FileStorage\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "with open(\"./dataset/rag_data.csv\", \"rb\") as f:\n",
    "    file = FileStorage(f)\n",
    "    file_content = file.read()\n",
    "    file_text = file_content.decode(\"utf-8\")\n",
    "\n",
    "    texts.append(file_text)\n",
    "    metadatas.append({'file_name': file.filename, 'file_id': str(uuid.uuid4())})\n",
    "\n",
    "# Print to verify\n",
    "print(texts)\n",
    "print(metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_texts_to_vectordb(texts, metadatas, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectordb(\"What causes Alstrom syndrome?\", device='cuda', n_vectordb=5, n_rerank=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "    Make sure you extract all the relevant keywords from the user's query.\n",
    "    Return output only as keywords separated by commas, without any explanations or additional text.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}\n",
    "    \n",
    "    ### Input:\n",
    "    {}\n",
    "    \n",
    "    ### Response:\n",
    "    {}\n",
    "\"\"\"\n",
    "\n",
    "def extract_keyword(model, tokenizer, text):\n",
    "    inputs =  tokenizer(\n",
    "            [\n",
    "                rag_prompt.format(\n",
    "                    \"Extract all the relevant keywords to store in vector database.\",\n",
    "                    text,\n",
    "                    \"\", # output\n",
    "                )\n",
    "            ],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "def rerank(device, query, candidates):\n",
    "    \"\"\"\n",
    "    Perform reranking of candidates based on cosine similarity with the query.\n",
    "    \n",
    "    :param query: The query sentence.\n",
    "    :param candidates: List of sentences to rerank.\n",
    "    :return: List of candidates reranked by similarity.\n",
    "    \"\"\"\n",
    "    print('?????????')\n",
    "\n",
    "    # Create embeddings for the query and candidates\n",
    "    query_embedding = embed([query], device)\n",
    "    candidate_embeddings = embed(candidates, device)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "    \n",
    "    # Sort indices by descending similarity scores\n",
    "    ranked_indices = np.argsort(cosine_scores.numpy())[::-1]\n",
    "    print(ranked_indices)\n",
    "    # Return the candidates sorted by similarity\n",
    "    reranked_candidates = [candidates[idx] for idx in ranked_indices]\n",
    "    \n",
    "    return reranked_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../lora_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = \"\"\"\n",
    "    You are Gaia - an AI assistant and answer or reply a whole conversation with your boss. \n",
    "    Your answer will include the answer to the user's question, the title of the document, and an exact citation of the text from that document,\n",
    "    or simply, reply your boss with the most relevant information from the document.\n",
    "    ### Instruction:\n",
    "    {}\n",
    "    \n",
    "    ### Input:\n",
    "    {}\n",
    "    \n",
    "    ### Response:\n",
    "    {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"What causes Alstrom syndrome?\"\n",
    "keywords = extract_keyword(model, tokenizer, inp)\n",
    "print(keywords)\n",
    "print('This is extremely slow')\n",
    "vectordb_results = query_vectordb(query=keywords, device=\"cuda\")\n",
    "\n",
    "sim_sentences = vectordb_results['documents']\n",
    "sim_scores = vectordb_results['scores']\n",
    "source_file_names = vectordb_results['file_names']\n",
    "\n",
    "print(\"\\n===== EXTRACTED SIM SENTENCES ============\")\n",
    "for sim_sentence, score in zip(sim_sentences, sim_scores):\n",
    "    print(f\"\\n== Sim sentences ({score}) == :\", sim_sentence)\n",
    "\n",
    "source_documents = '\\n\\n'.join([f\"- Document {f}:\" + s for f, s in zip(source_file_names, sim_sentences)])\n",
    "\n",
    "inputs = tokenizer(\n",
    "            [\n",
    "                final_prompt.format(\n",
    "                    \"Answer my question or chat with me: \" + inp,  # instruction\n",
    "                    \"The whole source documents which used in this conversation: \" + source_documents,  # input\n",
    "                    \"\",  # output - leave this blank for generation!\n",
    "                )\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
