{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golde/miniconda3/envs/deeplearning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3070 Ti. Max memory: 7.779 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What causes Alstrom syndrome?\n",
      " hopefully this will help you.\n",
      "What are the symptoms of Alstrom syndrome?\n",
      "What is the treatment for Alstrom syndrome?\n",
      "What is the prognosis of Alstrom syndrome?\n",
      "How can I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golde/miniconda3/envs/deeplearning/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/golde/miniconda3/envs/deeplearning/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/golde/miniconda3/envs/deeplearning/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None           # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True    # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "input_ids = tokenizer(\"What causes Alstrom syndrome?\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xDAN-datasets/medical_meadow_wikidoc_patient_information_6k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/golde/.cache/huggingface/datasets/xDAN-datasets___medical_meadow_wikidoc_patient_information_6k/default/0.0.0/e5fb4f4032e8d812a3d14d6dd886f530eb42a766 (last modified on Fri Sep 20 13:44:26 2024).\n",
      "Creating CSV from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 32.89ba/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "document = load_dataset(\"xDAN-datasets/medical_meadow_wikidoc_patient_information_6k\", split=\"train\")\n",
    "document.to_csv(\"./dataset/rag_data.csv\")\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./dataset\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Embedding & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Index with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some nodes are missing content, skipping them...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Tạo ChromaVectorStore, chỉ rõ nơi lưu (persist_directory) và tên collection\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "product_collection = chroma_client.get_or_create_collection(\"product_store\")\n",
    "chitchat_collection = chroma_client.get_or_create_collection(\"chitchat_store\")\n",
    "\n",
    "# Set up the ChromaVectorStore and StorageContext\n",
    "product_store = ChromaVectorStore(\n",
    "    persist_dir=\"./chromadb/chroma_db_product\",\n",
    "    collection_name=\"product_store\",\n",
    "    chroma_collection=product_collection,\n",
    ")\n",
    "\n",
    "chitchat_store = ChromaVectorStore(\n",
    "    persist_dir=\"./chromadb/chroma_db_chitchat\",\n",
    "    collection_name=\"chitchat_store\",\n",
    "    chroma_collection=chitchat_collection,\n",
    ")\n",
    "\n",
    "product_storage_context = StorageContext.from_defaults(vector_store=product_store)\n",
    "chitchat_storage_context = StorageContext.from_defaults(vector_store=chitchat_store)\n",
    "\n",
    "# Tạo Index, sử dụng ChromaVectorStore\n",
    "# (Ví dụ này, tất cả documents đều được index chung)\n",
    "product_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=Settings.transformations,\n",
    "    vector_store=product_store,\n",
    "    storage_context=product_storage_context\n",
    ")\n",
    "\n",
    "from llama_index.core import Document\n",
    "chitchat_docs = [\n",
    "    Document(content=\"Hello, how are you today?\"),\n",
    "    Document(content=\"What do you think about the weather?\"),\n",
    "    Document(content=\"Hey, have you watched any good movies lately?\"),\n",
    "    Document(content=\"What's your favorite hobby?\"),\n",
    "    Document(content=\"How's your day going?\"),\n",
    "    # Thêm nhiều tài liệu chitchat...\n",
    "]\n",
    "chitchat_index = VectorStoreIndex.from_documents(\n",
    "    chitchat_docs,\n",
    "    embed_model=embed_model,\n",
    "    transformations=Settings.transformations,\n",
    "    vector_store=chitchat_store,\n",
    "    storage_context=chitchat_storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The cause of Alstrom syndrome is a genetic mutation that affects the function of certain genes involved in the development and function of the body's organs. The mutation leads to the production of abnormal proteins that disrupt the normal function of the affected organs. The most common mutation is in the ALMS1 gene, which is responsible for the production of a protein called ALMS1. This protein is involved in the development and function of the eyes, ears, and kidneys. The mutation in the ALMS1 gene leads to the production of an abnormal protein that disrupts the normal function of these organs. The mutation can also affect other organs, such as the heart, liver, and spleen.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "product_query_engine = product_index.as_query_engine()\n",
    "chitchat_query_engine = chitchat_index.as_query_engine()\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "product_hyde_query = TransformQueryEngine(product_query_engine, hyde)\n",
    "response = product_hyde_query.query(\"What causes Alstrom syndrome?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "\n",
    "# list_tool = QueryEngineTool.from_defaults(\n",
    "#     query_engine=product_index,\n",
    "#     description=\"Useful for summarization questions related to the data source\",\n",
    "# )\n",
    "# vector_tool = QueryEngineTool.from_defaults(\n",
    "#     query_engine=chitchat_index,\n",
    "#     description=\"Useful for retrieving specific context related to the data source\",\n",
    "# )\n",
    "\n",
    "# # initialize router query engine (single selection, pydantic)\n",
    "# query_engine = RouterQueryEngine(\n",
    "#     selector=LLMSingleSelector.from_defaults(),\n",
    "#     query_engine_tools=[\n",
    "#         list_tool,\n",
    "#         vector_tool,\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# hyde_query_engine = TransformQueryEngine(\n",
    "#     query_engine=query_engine,\n",
    "#     query_transform=hyde,\n",
    "# )\n",
    "# hyde_query_engine.query(\"What causes Alstrom syndrome?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
