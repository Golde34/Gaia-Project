{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None           # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True    # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "input_ids = tokenizer(\"What causes Alstrom syndrome?\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "document = load_dataset(\"xDAN-datasets/medical_meadow_wikidoc_patient_information_6k\", split=\"train\")\n",
    "document.to_csv(\"./dataset/rag_data.csv\")\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./dataset\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Embedding & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Index with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Tạo ChromaVectorStore, chỉ rõ nơi lưu (persist_directory) và tên collection\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "product_collection = chroma_client.get_or_create_collection(\"product_store\")\n",
    "chitchat_collection = chroma_client.get_or_create_collection(\"chitchat_store\")\n",
    "\n",
    "# Set up the ChromaVectorStore and StorageContext\n",
    "product_store = ChromaVectorStore(\n",
    "    persist_dir=\"./chromadb/chroma_db_product\",\n",
    "    collection_name=\"product_store\",\n",
    "    chroma_collection=product_collection,\n",
    ")\n",
    "\n",
    "chitchat_store = ChromaVectorStore(\n",
    "    persist_dir=\"./chromadb/chroma_db_chitchat\",\n",
    "    collection_name=\"chitchat_store\",\n",
    "    chroma_collection=chitchat_collection,\n",
    ")\n",
    "\n",
    "product_storage_context = StorageContext.from_defaults(vector_store=product_store)\n",
    "chitchat_storage_context = StorageContext.from_defaults(vector_store=chitchat_store)\n",
    "\n",
    "# Tạo Index, sử dụng ChromaVectorStore\n",
    "# (Ví dụ này, tất cả documents đều được index chung)\n",
    "product_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=Settings.transformations,\n",
    "    vector_store=product_store,\n",
    "    storage_context=product_storage_context\n",
    ")\n",
    "\n",
    "from llama_index.core import Document\n",
    "chitchat_docs = [\n",
    "    Document(content=\"Hello, how are you today?\"),\n",
    "    Document(content=\"What do you think about the weather?\"),\n",
    "    Document(content=\"Hey, have you watched any good movies lately?\"),\n",
    "    Document(content=\"What's your favorite hobby?\"),\n",
    "    Document(content=\"How's your day going?\"),\n",
    "    # Thêm nhiều tài liệu chitchat...\n",
    "]\n",
    "chitchat_index = VectorStoreIndex.from_documents(\n",
    "    chitchat_docs,\n",
    "    embed_model=embed_model,\n",
    "    transformations=Settings.transformations,\n",
    "    vector_store=chitchat_store,\n",
    "    storage_context=chitchat_storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Semantic Router Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import ToolMetadata\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# choices as a list of tool metadata\n",
    "choices = [\n",
    "    ToolMetadata(description=\"Return medical patient information related to the query. \", name=\"product\"),\n",
    "    ToolMetadata(description=\"Chitchat\", name=\"chitchat\"),\n",
    "]\n",
    "\n",
    "# # choices as a list of strings\n",
    "# choices = [\n",
    "#     \"choice 1 - description for choice 1\",\n",
    "#     \"choice 2: description for choice 2\",\n",
    "# ]\n",
    "product_query_engine = product_index.as_query_engine()\n",
    "chitchat_query_engine = chitchat_index.as_query_engine()\n",
    "product_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=product_query_engine,\n",
    "    description=(\n",
    "        \"Return medical patient information related to the query. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "chitchat_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=chitchat_query_engine,\n",
    "    description=(\n",
    "        \"Return chitchat responses related to the query. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "query_engine_tools = [product_tool, chitchat_tool]\n",
    "\n",
    "selector = LLMSingleSelector.from_defaults()\n",
    "_metadatas = [x.metadata for x in query_engine_tools]\n",
    "selector_result = selector.select(\n",
    "    _metadatas, query=\"What causes Alstrom syndrome?\"\n",
    ")\n",
    "print(selector_result.selections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Semantic Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "product_query_engine = product_index.as_query_engine()\n",
    "chitchat_query_engine = chitchat_index.as_query_engine()\n",
    "\n",
    "product_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=product_query_engine,\n",
    "    description=(\n",
    "        \"Return medical patient information related to the query. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "chitchat_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=chitchat_query_engine,\n",
    "    description=(\n",
    "        \"Return chitchat responses related to the query. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        product_tool,\n",
    "        chitchat_tool,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# response = query_engine.query(\"Hello, how are you today?\")\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "product_hyde_query = TransformQueryEngine(query_engine, hyde)\n",
    "response = product_hyde_query.query(\"What causes Alstrom syndrome?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "\n",
    "# list_tool = QueryEngineTool.from_defaults(\n",
    "#     query_engine=product_index,\n",
    "#     description=\"Useful for summarization questions related to the data source\",\n",
    "# )\n",
    "# vector_tool = QueryEngineTool.from_defaults(\n",
    "#     query_engine=chitchat_index,\n",
    "#     description=\"Useful for retrieving specific context related to the data source\",\n",
    "# )\n",
    "\n",
    "# # initialize router query engine (single selection, pydantic)\n",
    "# query_engine = RouterQueryEngine(\n",
    "#     selector=LLMSingleSelector.from_defaults(),\n",
    "#     query_engine_tools=[\n",
    "#         list_tool,\n",
    "#         vector_tool,\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# hyde_query_engine = TransformQueryEngine(\n",
    "#     query_engine=query_engine,\n",
    "#     query_transform=hyde,\n",
    "# )\n",
    "# hyde_query_engine.query(\"What causes Alstrom syndrome?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. RAG (Retriever + LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "vector_retriever = VectorIndexRetriever(index=product_index, similarity_top_k=2)\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "query_str = \"What causes Alstrom syndrome?\"\n",
    "hyde_query_engine = TransformQueryEngine(vector_query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Semantic Splliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import (\n",
    "#     SentenceSplitter,\n",
    "#     SemanticSplitterNodeParser,\n",
    "# )\n",
    "\n",
    "# splitter = SemanticSplitterNodeParser(\n",
    "#     buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    "# )\n",
    "\n",
    "# # also baseline splitter\n",
    "# base_splitter = SentenceSplitter(chunk_size=512)\n",
    "# nodes = splitter.get_nodes_from_documents(documents)\n",
    "# print(f\"Number of nodes: {len(nodes)}\")\n",
    "# print(nodes[1].get_content())\n",
    "\n",
    "# base_nodes = base_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# vector_index = VectorStoreIndex(nodes)\n",
    "# query_engine = vector_index.as_query_engine()\n",
    "\n",
    "# base_vector_index = VectorStoreIndex(base_nodes)\n",
    "# base_query_engine = base_vector_index.as_query_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
